{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Generalization in Machine Learning\n",
    "\n",
    "Generalization refers to the ability of a machine learning model to perform well on new, unseen data, which was not used during the training process. The ultimate goal of a machine learning model is to make accurate predictions on future or unseen data, and generalization is a measure of how well a model achieves this goal.\n",
    "\n",
    "#### Importance of Generalization\n",
    "\n",
    "Generalization is crucial because it determines the practical usefulness of a model. A model that performs exceptionally well on training data but poorly on unseen data (a phenomenon known as overfitting) is of limited use in real-world applications. The true test of a model's effectiveness is its performance on new data, reflecting its ability to adapt to the complexities and variations it hasn't explicitly been trained on.\n",
    "\n",
    "#### Factors Affecting Generalization\n",
    "\n",
    "1. **Model Complexity**: More complex models with a large number of parameters can fit a wide variety of functions but are also more prone to overfitting. Simplifying the model or using techniques like regularization can help improve generalization.\n",
    "\n",
    "2. **Training Data Size**: Generally, the more training data available, the better the model can generalize. More data provides a more comprehensive representation of the underlying data distribution, reducing the model's chances of fitting to noise and specific peculiarities in the training set.\n",
    "\n",
    "3. **Noise in Data**: High noise levels in the training data can lead to poor generalization because the model might learn the noise as if it were a part of the signal. It's crucial to clean the data and possibly use noise reduction techniques during preprocessing.\n",
    "\n",
    "4. **Training Techniques**: Techniques such as cross-validation, regularization (like L1 and L2), and dropout in neural networks are designed to improve a model's generalization capabilities. These techniques help in preventing the model from becoming too fitted to the training data.\n",
    "\n",
    "#### Measuring Generalization\n",
    "\n",
    "Generalization is typically measured by the difference in performance (often error metrics like MSE, MAE, etc.) between the training set and a validation/test set. A small gap between training and test performance usually indicates good generalization.\n",
    "\n",
    "\n",
    "In summary, generalization is a fundamental concept in machine learning that not only guides the training process but also provides a benchmark for the model's future performance on unseen data. Ensuring good generalization is key to developing robust machine learning applications that perform well in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Error\n",
    "\n",
    "- **Training (Empirical) Error**:\n",
    "  $$\n",
    "  J_{\\text{train}}(w) = \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - h_w(x^{(i)}))^2\n",
    "  $$\n",
    "  This measures the average squared difference between the actual outputs and the predictions made by the model on the training data. It represents the model's accuracy on the data it was trained on.\n",
    "\n",
    "- **Test Error**:\n",
    "  $$\n",
    "  J_{\\text{test}}(w) = \\frac{1}{m} \\sum_{i=1}^m (y_{\\text{test}}^{(i)} - h_w(x_{\\text{test}}^{(i)}))^2\n",
    "  $$\n",
    "  This measures the average squared difference between the actual outputs and the predictions made by the model on new, unseen data. It is a measure of how well the model generalizes to new data.\n",
    "\n",
    "### Overfitting and Underfitting\n",
    "\n",
    "- **Overfitting**:\n",
    "  - **Description**: A model fits the training data very well but performs poorly on the test set.\n",
    "  - **Mathematical Expression**: $ J_{\\text{train}}(w) \\ll J_{\\text{test}}(w) $\n",
    "  - **Causes**: The model is too complex and has high variance.\n",
    "  - **Consequence**: The model captures noise in the training data and fails to generalize well to unseen data.\n",
    "\n",
    "- **Underfitting**:\n",
    "  - **Description**: The model is too simple and cannot capture the structure of the data.\n",
    "  - **Mathematical Expression**: $ J_{\\text{train}}(w) \\approx J_{\\text{test}}(w) \\gg 0 $\n",
    "  - **Causes**: The model lacks complexity and has high bias.\n",
    "  - **Consequence**: Poor fit on both training and test data, indicating that the model is not capable of capturing the underlying trends in the data.\n",
    "\n",
    "### Goal\n",
    "\n",
    "- The primary goal in model training is to minimize the test error, which helps in achieving good generalization on unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Decomposition\n",
    "\n",
    "### Generalization Error Decomposition\n",
    "\n",
    "The expected generalization error can be decomposed into three components:\n",
    "\n",
    "$$ \\mathbb{E}[(y-h_w(x))^2] = (\\text{Bias})^2 + \\text{Variance} + \\text{Noise} $$\n",
    "\n",
    "Where each component represents:\n",
    "\n",
    "#### 1. Bias\n",
    "Error due to simplifying assumptions in the model:\n",
    "$$ \\text{Bias}(x) = \\mathbb{E}[h_w(x)] - f(x) $$\n",
    "\n",
    "#### 2. Variance\n",
    "Sensitivity of the model to training data:\n",
    "$$ \\text{Variance}(x) = \\mathbb{E}[(h_w(x) - \\mathbb{E}[h_w(x)])^2] $$\n",
    "\n",
    "#### 3. Noise\n",
    "Irreducible error from the inherent randomness in data\n",
    "\n",
    "### Mathematical Proof\n",
    "\n",
    "Let's prove this decomposition step by step:\n",
    "\n",
    "#### Initial Assumptions\n",
    "- $f(x)$ is the ground truth\n",
    "- Observation $y$ is a noisy observation: $y = f(x) + \\epsilon$\n",
    "- $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$\n",
    "\n",
    "#### Step 1: Expected Squared Error\n",
    "We start with:\n",
    "$$ \\mathbb{E}_{data}[(\\hat{f}(x) - y)^2] = \\mathbb{E}_{data}[(\\hat{f}(x) - f(x) + \\epsilon)^2] $$\n",
    "\n",
    "This expands to:\n",
    "$$ = \\mathbb{E}[(\\hat{f}(x) - f(x))^2 - 2\\epsilon(\\hat{f}(x) - f(x)) + \\epsilon^2] $$\n",
    "\n",
    "#### Step 2: Noise Properties\n",
    "Since $\\epsilon$ has zero mean and variance $\\sigma^2$:\n",
    "- $\\mathbb{E}[\\epsilon] = 0$\n",
    "- $\\mathbb{E}[\\epsilon^2] = \\sigma^2$\n",
    "- $\\mathbb{E}[-2\\epsilon(\\hat{f}(x) - f(x))] = 0$\n",
    "\n",
    "#### Step 3: Decomposition\n",
    "We can decompose the squared difference $(\\hat{f}(x) - f(x))^2$ as:\n",
    "\n",
    "$$ \\mathbb{E}[(\\hat{f}(x) - f(x))^2] = \\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)] + \\mathbb{E}[\\hat{f}(x)] - f(x))^2] $$\n",
    "\n",
    "Expanding further:\n",
    "$$ = \\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2] + \\mathbb{E}[(\\mathbb{E}[\\hat{f}(x)] - f(x))^2] + 2\\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])(\\mathbb{E}[\\hat{f}(x)] - f(x))] $$\n",
    "\n",
    "Since $\\mathbb{E}[\\epsilon A] = \\mathbb{E}[\\epsilon]\\mathbb{E}[A]$, $A$ and $\\epsilon$ are independent and $\\mathbb{E}[\\epsilon]$ we have $\\mathbb{E}[\\epsilon A] = 0$ thus:\n",
    "\n",
    "$$ \\mathbb{E}[\\mathbb{E}[\\hat{f}(x)]-\\hat{f}(x)] = \\mathbb{E}[\\hat{f}(x)] - \\mathbb{E}[\\hat{f}(x)] = 0 $$\n",
    "\n",
    "#### now we have:\n",
    "$$ 2\\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])(\\mathbb{E}[\\hat{f}(x)] - f(x))] = 0$$\n",
    "\n",
    "#### thuth:\n",
    "$$ \n",
    "\\mathbb{E}_{data}[(\\hat{f}(x) - y)^2] = \\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2] + \\mathbb{E}[(\\mathbb{E}[\\hat{f}(x)] - f(x))^2] + \\epsilon^2\n",
    "$$\n",
    "\n",
    "#### Final Result\n",
    "Thus, the expected squared error becomes:\n",
    "\n",
    "$$ \\mathbb{E}_{data}[(\\hat{f}(x_n) - y)^2] = \\text{Variance} + \\text{Bias}^2 + \\sigma^2 $$\n",
    "\n",
    "where:\n",
    "- Variance = $\\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]$\n",
    "- Bias = $\\mathbb{E}[\\hat{f}(x)] - f(x)$\n",
    "###### $$ \\mathbb{E}[(\\mathbb{E}[\\hat{f}(x)] - f(x))^2] $$\n",
    "###### \n",
    "###### This expression represents the squared bias term in the bias-variance decomposition. Let's break down what it means:\n",
    "###### \n",
    "###### 1. $\\mathbb{E}[\\hat{f}(x)]$ is the expected value (average) of our model's predictions\n",
    "###### 2. $f(x)$ is the true function we're trying to approximate\n",
    "###### 3. $\\mathbb{E}[\\hat{f}(x)] - f(x)$ is the bias - how far off our model is on average\n",
    "###### 4. The outer $\\mathbb{E}[...]^2$ squares this difference to make it positive and penalize larger errors more heavily\n",
    "\n",
    "- Noise = $\\sigma^2$\n",
    "\n",
    "This decomposition helps us understand the trade-off between bias and variance in machine learning models, and why we can't eliminate all sources of error simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Analysis in Machine Learning Models\n",
    "\n",
    "### High Bias in Simple Models\n",
    "\n",
    "**Explanation**: Simple models, such as linear regression, often underfit\n",
    "\n",
    "$$ h_w(x) = w_0 + w_1x $$\n",
    "\n",
    "- Bias remains large even with infinite data\n",
    "$$ \\text{Bias}^2 \\gg \\text{Variance} $$\n",
    "\n",
    "- Leads to large generalization error\n",
    "\n",
    "### High Variance in Complex Models\n",
    "\n",
    "**Explanation**: Complex models tend to overfit\n",
    "\n",
    "$$ h_w(x) = w_0 + w_1x + w_2x^2 + \\cdots + w_mx^m $$\n",
    "\n",
    "- Variance dominates when the model is too complex\n",
    "$$ \\text{Variance} \\gg \\text{Bias} $$\n",
    "\n",
    "- Fits noise, leading to high test error\n",
    "\n",
    "### Bias-Variance Tradeoff\n",
    "\n",
    "**Tradeoff**: Balancing between bias and variance is key for optimal performance\n",
    "\n",
    "- Low complexity: High bias, low variance\n",
    "- High complexity: Low bias, high variance\n",
    "\n",
    "#### Key Points:\n",
    "1. Simple models (high bias):\n",
    "   - Underfitting\n",
    "   - Consistent but inaccurate predictions\n",
    "   - Adding more data doesn't help much\n",
    "\n",
    "2. Complex models (high variance):\n",
    "   - Overfitting\n",
    "   - Inconsistent predictions\n",
    "   - Very sensitive to training data\n",
    "\n",
    "3. Optimal model:\n",
    "   - Balances complexity\n",
    "   - Minimizes total error\n",
    "   - Appropriate for the problem complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization in Machine Learning\n",
    "\n",
    "### Purpose and Definition\n",
    "\n",
    "**Purpose**: Prevent overfitting by penalizing large weights\n",
    "\n",
    "$$ J_\\lambda(w) = J(w) + \\lambda R(w) $$\n",
    "\n",
    "- Common regularizers: L1 and L2 norms\n",
    "- $\\lambda$ controls the balance between fit and simplicity\n",
    "\n",
    "### Effect of Regularization Parameter $\\lambda$\n",
    "\n",
    "#### Balancing Fit and Complexity:\n",
    "\n",
    "$$ J_\\lambda(w) = J(w) + \\lambda \\sum_{j=1}^{m} w_j^2 = J(w) + \\lambda \\mathbf{w}^T\\mathbf{w} $$\n",
    "\n",
    "- **Large $\\lambda$**: Forces smaller weights, reduces complexity, increases bias, decreases variance\n",
    "- **Small $\\lambda$**: Allows larger weights, increases complexity, reduces bias, increases variance\n",
    "\n",
    "#### Mathematical Form for Polynomial Regression:\n",
    "\n",
    "![Image description](../../../Images/lamda-effect.png)\n",
    "\n",
    "$$ J_\\lambda(w) = \\sum_{i=1}^{n} (t^{(n)} - f(\\mathbf{x}^{(n)};\\mathbf{w}))^2 + \\lambda\\mathbf{w}^T\\mathbf{w} $$\n",
    "\n",
    "$$ f(\\mathbf{x}^{(n)};\\mathbf{w}) = w_0 + w_1x + \\cdots + w_9x^9 $$\n",
    "\n",
    "### Visual Evidence of Regularization Effects\n",
    "\n",
    "#### Impact on Model Fitting:\n",
    "\n",
    "- **High regularization (ln Œª = 0)**: Nearly linear fit, high bias\n",
    "- **Medium regularization (ln Œª = -18)**: Balanced fit capturing main trends\n",
    "- **Low regularization (ln Œª = -‚àû)**: Complex fit capturing noise (M = 9)\n",
    "\n",
    "#### Effect on Weight Values:\n",
    "\n",
    "| Weight | ln Œª = -‚àû | ln Œª = -18 | ln Œª = 0 |\n",
    "|--------|-----------|------------|----------|\n",
    "| w‚ÇÄ* | 0.35 | 0.35 | 0.13 |\n",
    "| w‚ÇÅ* | 232.37 | 4.74 | -0.05 |\n",
    "| w‚ÇÇ* | -5321.83 | -0.77 | -0.06 |\n",
    "| w‚ÇÉ* | 48568.31 | -31.97 | -0.05 |\n",
    "| ... | ... | ... | ... |\n",
    "| w‚Çâ* | 125201.43 | 72.68 | 0.01 |\n",
    "\n",
    "_Table adapted from Machine Learning and Pattern Recognition, Bishop_\n",
    "\n",
    "#### Regularization Parameter Optimization\n",
    "\n",
    "![Image description](../../../Images/ErmsForlamda.png)\n",
    "\n",
    "- $\\lambda$ controls the effective complexity of the model\n",
    "- Hence the degree of overfitting\n",
    "- Optimal Œª value is where test error is minimized (around ln Œª = -30 in graph)\n",
    "- Training error always increases with regularization, but test error forms a U-shape\n",
    "\n",
    "_Figures adapted from Machine Learning and Pattern Recognition, Bishop_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ **1. Ridge Regression (L2 Regularization)**\n",
    "### **Concept**\n",
    "Ridge regression adds an **L2 penalty** to the standard least squares loss function. This penalty is the **sum of the squared values of the model's coefficients**:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\sum (y_i - \\hat{y}_i)^2 + \\lambda \\sum w_j^2\n",
    "$$\n",
    "\n",
    "- The first term is the usual sum of squared errors.\n",
    "- The second term is the L2 penalty, where $\\lambda$ controls the strength of regularization.\n",
    "- **Larger $\\lambda$ values** shrink the coefficients closer to zero but never exactly zero.\n",
    "- Ridge regression helps when features are **highly correlated** (multicollinearity), distributing weights more evenly among them.\n",
    "\n",
    "### **Effect on Coefficients**\n",
    "- **Does not eliminate** coefficients (weights), just **reduces their magnitude**.\n",
    "- Helps reduce model complexity but keeps all features.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **2. Lasso Regression (L1 Regularization)**\n",
    "### **Concept**\n",
    "Lasso regression adds an **L1 penalty** to the loss function, which is the **sum of the absolute values of the coefficients**:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\sum (y_i - \\hat{y}_i)^2 + \\lambda \\sum |w_j|\n",
    "$$\n",
    "\n",
    "- The first term is the usual sum of squared errors.\n",
    "- The second term is the L1 penalty, which encourages some weights to be exactly **zero**.\n",
    "- **Larger $\\lambda$ values** result in more features being eliminated (coefficient = 0), effectively performing **feature selection**.\n",
    "\n",
    "### **Effect on Coefficients**\n",
    "- **Can set some coefficients to exactly zero**, making it useful for feature selection.\n",
    "- Helps simplify models and improve interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Comparison: Ridge vs. Lasso**\n",
    "| Feature | Ridge Regression | Lasso Regression |\n",
    "|---------|----------------|----------------|\n",
    "| Regularization Type | L2 (squared weights) | L1 (absolute weights) |\n",
    "| Effect on Coefficients | Shrinks but does not eliminate | Can eliminate (set to zero) |\n",
    "| When to Use? | If you want to **reduce** weights but keep all features | If you want to **select important features** |\n",
    "| Best for | **Multicollinearity** (highly correlated features) | **Sparse models** (few significant predictors) |\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **3. Elastic Net (Combination of Ridge & Lasso)**\n",
    "Since Ridge and Lasso have different strengths, **Elastic Net** combines both:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\sum (y_i - \\hat{y}_i)^2 + \\lambda_1 \\sum |w_j| + \\lambda_2 \\sum w_j^2\n",
    "$$\n",
    "\n",
    "- Uses **both L1 and L2 penalties**.\n",
    "- Useful when **features are correlated and some feature selection is needed**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **5. When to Use Ridge or Lasso?**\n",
    "- Use **Ridge** if you have **many correlated variables** and want to keep them all.\n",
    "- Use **Lasso** if you want **automatic feature selection** and a simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
