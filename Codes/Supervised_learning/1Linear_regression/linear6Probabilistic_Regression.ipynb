{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Regression (Probabilistic Perspective)\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "* **Objective:** Model the relationship between input $\\mathbf{x}$ and output $y$.\n",
    "\n",
    "* **Uncertainty:** Output $y$ has an associated uncertainty modeled by a probability distribution.\n",
    "\n",
    "* **Example:**\n",
    "  \n",
    "  $y = f(\\mathbf{x}; \\mathbf{w}) + \\epsilon$ , where $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$\n",
    "\n",
    "* The goal is to learn $f(\\mathbf{x}; \\mathbf{w})$ to predict $y$.\n",
    "\n",
    "## Curve Fitting with Noise\n",
    "\n",
    "* In real-world scenarios, observed output $y$ is noisy.\n",
    "\n",
    "* **Model: True output plus noise**\n",
    "  \n",
    "  $y = f(\\mathbf{x}; \\mathbf{w}) + \\epsilon$\n",
    "\n",
    "* Noise represents unknown or unmodeled factors.\n",
    "\n",
    "* **Example:** Predicting house prices based on features with inherent unpredictability.\n",
    "\n",
    "## Expected Value of Output\n",
    "\n",
    "* **Best Estimate:** The conditional expectation of $y$ given $\\mathbf{x}$.\n",
    "  \n",
    "  $\\mathbb{E}[y|\\mathbf{x}] = f(\\mathbf{x}; \\mathbf{w})$\n",
    "\n",
    "* **Goal:** Learn a function $f(\\mathbf{x}; \\mathbf{w})$ that represents the average behavior of the data.\n",
    "\n",
    "* **Key Point:** The model captures the mean of the target variable given input $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîπ **What is Likelihood?**\n",
    "Likelihood is a measure of how probable it is to observe some given data under a particular model or set of parameters.  \n",
    "\n",
    "Mathematically, the **likelihood function** is written as:\n",
    "\n",
    "$$\n",
    "L(\\theta) = P(D | \\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $L(\\theta)$ is the **likelihood** of the parameters $\\theta$.\n",
    "- $D$ is the observed **data**.\n",
    "- $P(D | \\theta)$ is the probability of observing $D$ **given** the parameters $\\theta$.\n",
    "\n",
    "üëâ **Key idea:** **Likelihood is not the probability of parameters; it is the probability of the data given the parameters.**  \n",
    "\n",
    "### üîπ **Example of Likelihood**\n",
    "Imagine you flip a coin 10 times and observe 7 heads and 3 tails. You want to find out how likely it is that the coin is biased (i.e., not fair).  \n",
    "\n",
    "Let's define:\n",
    "- $\\theta$ as the probability of heads.\n",
    "- The observed data is **7 heads and 3 tails**.\n",
    "\n",
    "The likelihood function is:\n",
    "\n",
    "$$\n",
    "L(\\theta) = P(D | \\theta) = \\theta^7 (1 - \\theta)^3\n",
    "$$\n",
    "\n",
    "This function tells us how **likely** different values of $\\theta$ (the probability of heads) are, given the observed data.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **What is Maximum Likelihood Estimation (MLE)?**\n",
    "**Maximum Likelihood Estimation (MLE)** is a method used to find the **best** value for a parameter ($\\theta$) by maximizing the likelihood function.\n",
    "\n",
    "üëâ **The goal of MLE:**  \n",
    "Find the parameter value $\\theta$ that makes the observed data **most likely**.\n",
    "\n",
    "### üîπ **Mathematical Definition of MLE**\n",
    "The **MLE estimate** for $\\theta$ is the value that **maximizes** the likelihood function:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\arg\\max_{\\theta} L(\\theta)\n",
    "$$\n",
    "\n",
    "Since likelihood functions often involve **products of probabilities**, it is easier to work with the **log-likelihood** (taking the natural logarithm):\n",
    "\n",
    "$$\n",
    "\\log L(\\theta) = \\sum \\log P(D_i | \\theta)\n",
    "$$\n",
    "\n",
    "Maximizing the **log-likelihood** instead of the likelihood is common because:\n",
    "- It converts products into **sums**, making calculations easier.\n",
    "- The logarithm is a **monotonic function**, so maximizing likelihood and maximizing log-likelihood are equivalent.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Example: MLE for a Coin Flip**\n",
    "Suppose we flip a coin $n$ times and get $k$ heads. We want to estimate the probability of heads ($\\theta$) using MLE.\n",
    "\n",
    "1Ô∏è‚É£ **Likelihood function**:\n",
    "$$\n",
    "L(\\theta) = \\theta^k (1 - \\theta)^{n-k}\n",
    "$$\n",
    "\n",
    "2Ô∏è‚É£ **Log-likelihood function**:\n",
    "$$\n",
    "\\log L(\\theta) = k \\log \\theta + (n-k) \\log (1 - \\theta)\n",
    "$$\n",
    "\n",
    "3Ô∏è‚É£ **Find $\\theta$ that maximizes this function**:  \n",
    "To maximize, take the derivative and set it to zero:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\theta} [ k \\log \\theta + (n-k) \\log (1 - \\theta) ] = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\theta$, we get:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{k}{n}\n",
    "$$\n",
    "\n",
    "üëâ **Conclusion**: The MLE for the probability of heads in a coin flip is just the fraction of times we observed heads!\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Why Use MLE?**\n",
    "MLE is widely used in statistics and machine learning because:\n",
    "‚úÖ It provides **consistent** estimates as data increases.  \n",
    "‚úÖ It works for a wide range of probability distributions.  \n",
    "‚úÖ It is the foundation of many machine learning algorithms (e.g., logistic regression, Gaussian mixture models).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "* **MLE**: A method to estimate parameters that maximize the likelihood of the data.\n",
    "\n",
    "* Given data $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, MLE maximizes:\n",
    "\n",
    "$$\n",
    "L(\\mathcal{D};\\mathbf{w}, \\sigma^2) = \\prod_{i=1}^n p(y_i|\\mathbf{x}_i, \\mathbf{w}, \\sigma^2)\n",
    "$$\n",
    "\n",
    "* MLE finds parameters $\\mathbf{w}$ and $\\sigma^2$ that best explain the data.\n",
    "\n",
    "## Log-Likelihood\n",
    "\n",
    "* Instead of maximizing the likelihood, it is often easier to maximize the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\log L(\\mathcal{D};\\mathbf{w}, \\sigma^2) = \\sum_{i=1}^n \\log p(y_i|\\mathbf{x}_i, \\mathbf{w}, \\sigma^2)\n",
    "$$\n",
    "\n",
    "* It is because $\\log f(x)$ preserves the behaviour of $f(x)$.\n",
    "* It is also easier to find derivative on summation of terms.\n",
    "\n",
    "## Univariate Linear Function Example\n",
    "\n",
    "* Assuming Gaussian noise with parameters $(0,\\sigma^2)$, probability of observing real output value $y$ is:\n",
    "\n",
    "$$\n",
    "p(y|\\mathbf{x}, \\mathbf{w}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-f(\\mathbf{x};\\mathbf{w}))^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "* For a simple linear model $f(\\mathbf{x};\\mathbf{w}) = w_0 + w_1x$ we have:\n",
    "\n",
    "$$\n",
    "p(y|x, \\mathbf{w}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-w_0-w_1x)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "**Key Observation**: Points far from the fitted line will have a low likelihood value.\n",
    "\n",
    "## Log-Likelihood and Sum of Squares\n",
    "\n",
    "* Using log-likelihood we have:\n",
    "\n",
    "$$\n",
    "\\log L(\\mathcal{D};\\mathbf{w}, \\sigma^2) = -n\\log\\sigma - \\frac{n}{2}\\log(2\\pi) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y^{(i)} - f(\\mathbf{x}^{(i)};\\mathbf{w}))^2\n",
    "$$\n",
    "\n",
    "* Since the objective of MLE is to optimize with regards to random variables, we can rule out the constants:\n",
    "\n",
    "$$\n",
    "\\log L(\\mathcal{D};\\mathbf{w}, \\sigma^2) \\sim -\\sum_{i=1}^n(y^{(i)} - f(\\mathbf{x}^{(i)};\\mathbf{w}))^2\n",
    "$$\n",
    "\n",
    "* **Equivalence**: Maximizing the log-likelihood is equivalent to minimizing the Sum of Squared Errors (SSE):\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\sum_{i=1}^n(y^{(i)} - f(\\mathbf{x}^{(i)};\\mathbf{w}))^2\n",
    "$$\n",
    "\n",
    "## Estimating $\\sigma^2$\n",
    "\n",
    "* The maximum likelihood estimate of the noise variance $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n(y^{(i)} - f(\\mathbf{x}^{(i)};\\hat{\\mathbf{w}}))^2\n",
    "$$\n",
    "\n",
    "* **Interpretation**: Mean squared error of the predictions.\n",
    "* **Note**: $\\sigma^2$ reflects the noise level in the observations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
