{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Classification:\n",
    "\n",
    "## Definition\n",
    "Classification is a supervised learning task where the model learns to predict discrete categories (labels) from input data.\n",
    "\n",
    "### Simple Example\n",
    "- **Input**: Features of an email (words, sender, subject)\n",
    "- **Output**: Category (Spam or Not Spam)\n",
    "\n",
    "### Key Points\n",
    "1. **Discrete Output**: Unlike regression which predicts continuous values, classification predicts distinct categories\n",
    "2. **Types**:\n",
    "   - Binary Classification (2 classes)\n",
    "   - Multi-class Classification (>2 classes)\n",
    "\n",
    "### Mathematical Form\n",
    "Given input $\\mathbf{x}$, learn a function $f$ that maps to a class label $y$:\n",
    "$$f: \\mathbf{x} \\rightarrow y \\text{ where } y \\in \\{1,2,...,K\\}$$\n",
    "where $K$ is the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Discriminant Functions in Machine Learning\n",
    "\n",
    "## Basic Definition\n",
    "A discriminant function is a mathematical tool that helps classify input data into different categories. Let's understand its key components:\n",
    "\n",
    "### üéØ Definition\n",
    "* A discriminant function assigns a score to an input vector $\\mathbf{x}$ to classify it into different classes\n",
    "* It maps input $\\mathbf{x}$ to a real number $g(\\mathbf{x})$, representing the confidence in assigning $\\mathbf{x}$ to a particular class\n",
    "\n",
    "### üîÑ How it Works\n",
    "\n",
    "#### Binary Classification\n",
    "For problems with two classes $C_1$ and $C_2$, we use two functions $g_1(\\mathbf{x})$ and $g_2(\\mathbf{x})$. The prediction rule is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\begin{cases}\n",
    "C_1 & \\text{if } g_1(\\mathbf{x}) > g_2(\\mathbf{x}) \\\\\n",
    "C_2 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### General Case\n",
    "For $k$-class problems, we compute $g_i(\\mathbf{x})$ for every class $i$, and assign to the class with highest score:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_i g_i(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "## üöß Decision Boundaries\n",
    "\n",
    "A decision boundary is a dividing hyperplane that separates different classes in a feature space. It's also known as a \"Decision Surface\".\n",
    "\n",
    "### Types of Decision Boundaries\n",
    "\n",
    "#### 2D Decision Boundaries\n",
    "![2D Decision Boundaries](../../../Images/2D_linear_boundary.png)\n",
    "*Linear decision boundary in 2D space*\n",
    "\n",
    "![2D Non-linear Decision Boundary](../../../Images/2D_nonlinear_boundary.png)\n",
    "*Non-linear decision boundary in 2D space*\n",
    "\n",
    "#### 3D Decision Boundaries\n",
    "![3D Linear Decision Boundary](../../../Images/3D_linear_boundary.png)\n",
    "*Linear decision boundary in 3D space*\n",
    "\n",
    "![3D Non-linear Decision Boundary](../../../Images/3D_nonlinear_boundary.png)\n",
    "*Non-linear decision boundary in 3D space*\n",
    "\n",
    "## üìä Two-Category Classification\n",
    "\n",
    "For binary classification problems:\n",
    "\n",
    "1. **Function**: We need only one function $g: \\mathbb{R}^d \\to \\mathbb{R}$ where:\n",
    "   * $g_1(\\mathbf{x}) = g(\\mathbf{x})$\n",
    "   * $g_2(\\mathbf{x}) = -g(\\mathbf{x})$\n",
    "\n",
    "2. **Decision Boundary**: The boundary is defined by $g(\\mathbf{x}) = 0$\n",
    "\n",
    "3. **Key Point**: We start with binary classification for simplicity before extending to multi-category classification.\n",
    "\n",
    "### üí° Important Notes:\n",
    "- The decision boundary shape depends on the type of discriminant function used\n",
    "- Linear boundaries are simpler but may not be suitable for complex data\n",
    "- Non-linear boundaries can capture more complex patterns but might risk overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifiers\n",
    "#### Definition\n",
    "In case of linear classifiers, decision boundaries are linear in $d$ ($\\mathbf{x} \\in \\mathbb{R}^d$), or linear in some given set of functions of $x$.\n",
    "\n",
    "#### Key Concepts\n",
    "* **Linearly separable data**: Data points that can be exactly separated by a linear decision boundary.\n",
    "* **Why are they popular?** \n",
    "  - Simplicity\n",
    "  - Efficiency\n",
    "  - Effectiveness\n",
    "\n",
    "### Two Category Classification\n",
    "The linear discriminant function is defined as:\n",
    "\n",
    "$$g(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x} + w_0 = w_d \\cdot x_d + ... + w_1 \\cdot x_1 + w_0$$\n",
    "\n",
    "where:\n",
    "* $\\mathbf{x} = [x_1 ... x_d]$ (input features)\n",
    "* $\\mathbf{w} = [w_1 ... w_d]$ (weights)\n",
    "* $w_0$: bias term\n",
    "\n",
    "The classification rule is:\n",
    "$$\n",
    "\\begin{cases}\n",
    "C_1 & \\text{if } \\mathbf{w}^T\\mathbf{x} + w_0 \\geq 0 \\\\\n",
    "C_2 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Decision Boundary Properties\n",
    "The decision boundary is a $(d-1)$-dimensional hyperplane $H$ in the $d$-dimensional feature space. Properties:\n",
    "* Orientation of $H$ is determined by the normal vector $[\\frac{w_1}{\\|\\mathbf{w}\\|}, ..., \\frac{w_d}{\\|\\mathbf{w}\\|}]$\n",
    "* $w_0$ determines the location of the surface\n",
    "\n",
    "![Determines Location Of Surface](../../../Images/Determines_Location_Of_Surface.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Non-linear Decision Boundaries\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1Ô∏è‚É£ Feature Transformation\n",
    "* Transforms features into a higher-dimensional space\n",
    "* Introduces non-linearity to the model\n",
    "* Example transformation: $\\phi(\\mathbf{x}) = [1, x_1, x_2, x_1^2, x_2^2, x_1x_2]$\n",
    "\n",
    "### 2Ô∏è‚É£ Linear in Transformed Space\n",
    "* Decision boundary becomes:\n",
    "  * Linear in the new transformed space\n",
    "  * Non-linear in the original space\n",
    "* Example equation: $-1 + x_1^2 + x_2^2 = 0$ (creates a circular boundary)\n",
    "\n",
    "### 3Ô∏è‚É£ Mathematical Representation\n",
    "* Original input: $\\mathbf{x} = [x_1, x_2]$\n",
    "* Transformation: $\\phi(\\mathbf{x}) = [1, x_1, x_2, x_1^2, x_2^2, x_1x_2]$\n",
    "* Weights: $\\mathbf{w} = [w_0, w_1, ..., w_m] = [-1, 0, 0, 1, 1, 0]$\n",
    "\n",
    "### 4Ô∏è‚É£ Classification Rule\n",
    "```python\n",
    "if w^T œÜ(x) ‚â• 0 then y = 1\n",
    "else y = -1\n",
    "```\n",
    "![3D Determines Location Of Surface](../../../Images/3D_Determines_Location_Of_Surface.png)Determines_Location_Of_Surface\n",
    "\n",
    "### üí° Key Point\n",
    "The transformation allows us to create complex decision boundaries (like circles) while still using linear classification techniques in the transformed space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
